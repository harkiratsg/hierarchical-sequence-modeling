import numpy as npimport torchfrom torch import nnimport torch.nn.functional as Ffrom torch.autograd import Variableimport osimport stringimport redef tokenize(text, char2int):    # Lowercase text and remove multiple spaces    text = text.lower()    text = text.replace("\n", " ")    text = re.sub(' +', ' ', text)        # Clean text    text = re.sub('[^' + chars + ']', '', text)    # Tokenize text    tokenized = np.array([char2int[char] for char in text])    return tokenized# read and prepare the datawith open('./nested_abcxyz.txt', 'r') as f:    text = f.read()# Only keep alphabet, spaces, and special tokensnon_alpha_chars = "#"chars = string.ascii_lowercase + " " + non_alpha_charsint2char = dict(enumerate(tuple(set(chars))))char2int = {char: index for index, char in int2char.items()}tokenized = tokenize(text, char2int)def one_hot_encode(arr, n_labels):    # Initialize the the encoded array    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)        # Fill the appropriate elements with ones    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.        # Finally reshape it to get back to the original array    one_hot = one_hot.reshape((*arr.shape, n_labels))        return one_hotdef get_batches(arr, n_seqs_in_a_batch, n_characters):    batch_size = n_seqs_in_a_batch * n_characters    n_batches = len(arr)//batch_size        # Keep only enough characters to make full batches    arr = arr[:n_batches * batch_size]    # Reshape into n_seqs rows    arr = arr.reshape((n_seqs_in_a_batch, -1))        for n in range(0, arr.shape[1], n_characters):        # The features        x = arr[:, n:n+n_characters]        # The targets, shifted by one        y = np.zeros_like(x)        try:            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_characters]        except IndexError:            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]        yield x, y        # check if GPU is availabletrain_on_gpu = torch.cuda.is_available()if(train_on_gpu):    print('Training on GPU')else:     print('No GPU available')            class CharLSTM(nn.Module):      def __init__(self, n_tokens, n_hidden, n_layers=1, drop_prob=0.5, lr=0.001):        super().__init__()        self.drop_prob = drop_prob        self.n_layers = n_layers        self.n_hidden = n_hidden        self.lr = lr        self.lstm = nn.LSTM(n_tokens, n_hidden, n_layers, dropout=drop_prob, batch_first=True)        self.dropout = nn.Dropout(drop_prob)        self.fc = nn.Linear(n_hidden, n_tokens)              def forward(self, x, hidden):         out, hidden = self.lstm(x, hidden)        out = self.dropout(out)        out = out.contiguous().view(-1, self.n_hidden)        y = self.fc(out)        return y, hidden            def init_hidden(self, batch_size):        return (torch.zeros(self.n_layers, batch_size, self.n_hidden),                torch.zeros(self.n_layers, batch_size, self.n_hidden))def train(model, data, n_epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):    model.train()        optim = torch.optim.Adam(net.parameters(), lr=lr)    loss_fn = nn.CrossEntropyLoss()        # create training and validation data    val_idx = int(len(data)*(1-val_frac))    data, val_data = data[:val_idx], data[val_idx:]        if(train_on_gpu):        net.cuda()        n_chars = len(int2char)    for e in range(n_epochs):        # initialize hidden state        h = model.init_hidden(batch_size)                batch_update_steps = 0        total_epoch_loss = 0        for x, y in get_batches(data, batch_size, seq_length):            batch_update_steps += 1                        # One-hot encode our data and make them Torch tensors            x = one_hot_encode(x, n_chars)            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)                        if(train_on_gpu):                inputs, targets = inputs.cuda(), targets.cuda()            # zero accumulated gradients            net.zero_grad()                        # get the output from the model            output, hidden = model(inputs, h)                        # Backpropagate loss            loss = loss_fn(output, targets.view(batch_size*seq_length))            loss.backward()            # Clip gradients            nn.utils.clip_grad_norm_(model.parameters(), clip)            optim.step()            # Clone hidden so it breaks from existing computational graph for next update            hidden = tuple(([Variable(var.data) for var in hidden]))            total_epoch_loss += loss.item()                    avg_epoch_loss = total_epoch_loss / batch_update_steps        print("Epoch {}/{}: Avg loss: {}".format(e+1, n_epochs, avg_epoch_loss))chars = "".join(list(char2int.keys()))print(chars)# define and print the netn_hidden=20n_layers=1net = CharLSTM(len(int2char.keys()), n_hidden, n_layers)print(net)batch_size = 1seq_length = 10 #max length versesn_epochs = 10 # start smaller if you are just testing initial behavior# train the modeltrain(net, tokenized, n_epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=1000)def predict(net, char, h=None, top_k=None):        ''' Given a character, predict the next character.            Returns the predicted character and the hidden state.        '''                # tensor inputs        x = np.array([[char2int[char]]])        x = one_hot_encode(x, len(int2char))        inputs = torch.from_numpy(x)                if(train_on_gpu):            inputs = inputs.cuda()                # detach hidden state from history        h = tuple([each.data for each in h])        # get the output of the model        out, h = net(inputs, h)        # get the character probabilities        # apply softmax to get p probabilities for the likely next character giving x        p = F.softmax(out, dim=1).data        if(train_on_gpu):            p = p.cpu() # move to cpu                # get top characters        # considering the k most probable characters with topk method        if top_k is None:            top_ch = np.arange(len(net.chars))        else:            p, top_ch = p.topk(top_k)            top_ch = top_ch.numpy().squeeze()                # select the likely next character with some element of randomness        p = p.numpy().squeeze()        char = np.random.choice(top_ch, p=p/p.sum())                # return the encoded value of the predicted char and the hidden state        return int2char[char], hdef sample(net, size, prime='Il', top_k=None):            if(train_on_gpu):        net.cuda()    else:        net.cpu()        net.eval() # eval mode        # First off, run through the prime characters    chars = [ch for ch in prime]    h = net.init_hidden(1)    for ch in prime:        char, h = predict(net, ch, h, top_k=top_k)    chars.append(char)        # Now pass in the previous character and get a new one    for ii in range(size):        char, h = predict(net, chars[-1], h, top_k=top_k)        chars.append(char)    return ''.join(chars)print(sample(net, 1000, prime='y', top_k=2))