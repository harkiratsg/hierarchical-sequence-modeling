import numpy as npimport torchfrom torch import nnimport torch.nn.functional as Ffrom torch.autograd import Variableimport stringimport reclass Tokenizer():    def __init__(self, non_alpha_chars):        self.chars = string.ascii_lowercase + " " + non_alpha_chars        self.int2char = dict(enumerate(tuple(set(self.chars))))        self.char2int = {char: index for index, char in self.int2char.items()}        def tokenize(self, text):        return [self.char2int[char] for char in text]        def to_text(self, tokenized):        return "".join([self.int2char[i] for i in tokenized])def clean_text(text, chars):        # Lowercase text and remove multiple spaces        text = text.lower()        text = text.replace("\n", " ")        text = re.sub(' +', ' ', text)        text = re.sub('[^' + chars + ']', '', text)        return text        def one_hot_encode(arr, n_labels):    # Initialize the the encoded array    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)        # Fill the appropriate elements with ones    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.        # Finally reshape it to get back to the original array    one_hot = one_hot.reshape((*arr.shape, n_labels))        return one_hotdef get_batches(arr, n_seqs_in_a_batch, n_characters):    batch_size = n_seqs_in_a_batch * n_characters    n_batches = len(arr)//batch_size        # Keep only enough characters to make full batches    arr = arr[:n_batches * batch_size]     # Reshape into n_seqs rows    arr = arr.reshape((n_seqs_in_a_batch, -1))        x_batch = []    y_batch = []    for n in range(0, arr.shape[1], n_characters):        # The features        x = arr[:, n:n+n_characters]        # The targets, shifted by one        y = np.zeros_like(x)        try:            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_characters]        except IndexError:            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]                        x_batch.append(x)        y_batch.append(y)        return np.stack(x_batch, axis=0), np.stack(y_batch, axis=0)def split_text_seq(text, s, p):    text_seqs = text.split(s)    return "".join(text_seqs[:int(p * len(text_seqs))]), "".join(text_seqs[int(p * len(text_seqs)):])# Dont use this it doesnt work. I'm just going to keep it here if itt may be needed in the futuredef split_seq(seq, p, split_token):    sub_seqs = np.split(seq, np.where(seq == split_token)[0] + 1)        # Remove last list if empty    if len(sub_seqs[-1]) == 0:        del sub_seqs[-1]        i = int(p * len(sub_seqs))        if i == 0:        return np.array([]), seq    elif i == len(sub_seqs):        return seq, np.array([])            return np.concatenate(sub_seqs[:i], axis=0), np.concatenate(sub_seqs[i:], axis=0)